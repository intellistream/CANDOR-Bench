#!/usr/bin/env python3
"""
benchmark_anns Main Runner

ä¸»è¿è¡Œç¨‹åºï¼Œç”¨äºæ‰§è¡Œæµå¼ç´¢å¼•åŸºå‡†æµ‹è¯•
è®¾è®¡å‚è€ƒ big-ann-benchmarks/run.py å’Œ benchmark/runner.py

ç”¨æ³•ç¤ºä¾‹ï¼š
    # åŸºç¡€ç”¨æ³•
    python run_benchmark.py --algorithm faiss_hnsw --dataset sift --runbook batch_sizes/batch_2500
    
    # æŒ‡å®šè¾“å‡ºç›®å½•
    python run_benchmark.py --algorithm faiss_hnsw --dataset sift --runbook batch_sizes/batch_2500 --output results/exp1
    
    # æŒ‡å®šç®—æ³•å‚æ•°ï¼ˆJSON æ ¼å¼ï¼‰
    python run_benchmark.py --algorithm faiss_hnsw --dataset sift --runbook event_rates/rate_10000 \
        --algo-params '{"M": 32, "efConstruction": 200, "efSearch": 100}'
    
    # å¤šæ¬¡è¿è¡Œå–æœ€ä½³ç»“æœ
    python run_benchmark.py --algorithm faiss_hnsw --dataset sift --runbook batch_sizes/batch_2500 --runs 3
    
    # åˆ—å‡ºå¯ç”¨é€‰é¡¹
    python run_benchmark.py --list-algorithms
    python run_benchmark.py --list-datasets
    python run_benchmark.py --list-runbooks
"""

import os
import sys
import argparse
import json
import yaml
import time
import traceback
import re
import h5py
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple
from datetime import datetime

# benchmark_anns æ˜¯ç‹¬ç«‹é¡¹ç›®ï¼Œä½¿ç”¨ç›¸å¯¹å¯¼å…¥
from bench.algorithms.registry import get_algorithm, auto_register_algorithms, ALGORITHMS, get_algorithm_params_from_config, get_all_algorithm_param_combinations
from datasets.registry import get_dataset, DATASETS
from bench.runner import BenchmarkRunner
from bench.metrics import BenchmarkMetrics


def _extract_key_params(params: Dict[str, Any], max_depth: int = 4) -> Dict[str, Any]:
    """
    ä»åµŒå¥—å‚æ•°ä¸­æå–å…³é”®å‚æ•°ç”¨äºç”Ÿæˆæ–‡ä»¶å¤¹å
    
    Args:
        params: å‚æ•°å­—å…¸ï¼ˆå¯èƒ½åµŒå¥—ï¼‰
        max_depth: æœ€å¤§é€’å½’æ·±åº¦
        
    Returns:
        æ‰å¹³åŒ–çš„å…³é”®å‚æ•°å­—å…¸
    """
    result = {}
    
    # æˆ‘ä»¬å…³å¿ƒçš„å…³é”®å‚æ•°
    key_params = {
        'max_degree', 'M', 'ef_construction', 'efConstruction', 
        'ef_search', 'efSearch', 'prefetch_mode', 'nlist', 'nprobe',
        'search_L', 'index_L', 'R', 'L', 'alpha'
    }
    
    def _flatten(d: Dict[str, Any], prefix: str = "", depth: int = 0):
        if depth >= max_depth:
            return
        for key, value in d.items():
            new_key = f"{prefix}{key}" if prefix else key
            if isinstance(value, dict):
                _flatten(value, f"{new_key}_", depth + 1)
            elif isinstance(value, (str, int, float, bool)):
                # åªä¿ç•™å…³é”®å‚æ•°ï¼Œæˆ– prefetch_mode è¿™ç±»ç‰¹æ®Šå‚æ•°
                if key in key_params or 'prefetch' in key.lower():
                    result[key] = value  # ä½¿ç”¨ç®€çŸ­çš„ keyï¼Œä¸å¸¦å‰ç¼€
    
    _flatten(params)
    return result


def _generate_params_folder_name(algorithm_params: Dict[str, Any]) -> str:
    """
    æ ¹æ®ç®—æ³•å‚æ•°ç”Ÿæˆæœ‰æ„ä¹‰çš„æ–‡ä»¶å¤¹å
    
    Args:
        algorithm_params: åŒ…å« build_params å’Œ query_params çš„å­—å…¸
        
    Returns:
        æ–‡ä»¶å¤¹åï¼ˆå¦‚ "M32_ef200_prefetch-hardcoded_efsearch40"ï¼‰
    """
    if not algorithm_params:
        return "default"
    
    parts = []
    
    # å¤„ç†æ„å»ºå‚æ•°
    build_params = algorithm_params.get('build_params', {})
    if build_params:
        flat_build = _extract_key_params(build_params)
        for key, value in sorted(flat_build.items()):
            # ç®€åŒ–å‚æ•°å
            short_key = key.replace('max_degree', 'M').replace('ef_construction', 'ef')
            short_key = short_key.replace('efConstruction', 'ef').replace('prefetch_mode', 'prefetch')
            
            # æ ¼å¼åŒ–å€¼
            if isinstance(value, bool):
                if value:
                    parts.append(short_key)
            elif isinstance(value, float):
                parts.append(f"{short_key}{value:.0f}")
            else:
                parts.append(f"{short_key}-{value}")
    
    # å¤„ç†æŸ¥è¯¢å‚æ•°
    query_params = algorithm_params.get('query_params', {})
    if query_params:
        flat_query = _extract_key_params(query_params)
        for key, value in sorted(flat_query.items()):
            short_key = key.replace('ef_search', 'efsearch').replace('efSearch', 'efsearch')
            
            if isinstance(value, bool):
                if value:
                    parts.append(short_key)
            elif isinstance(value, float):
                parts.append(f"{short_key}{value:.0f}")
            else:
                parts.append(f"{short_key}-{value}")
    
    if not parts:
        return "default"
    
    # ç»„åˆå¹¶é™åˆ¶é•¿åº¦
    folder_name = "_".join(parts)
    
    # æ¸…ç†éæ³•å­—ç¬¦
    folder_name = re.sub(r'[^\w\-]', '_', folder_name)
    folder_name = re.sub(r'_+', '_', folder_name).strip('_')
    
    # é™åˆ¶é•¿åº¦
    if len(folder_name) > 100:
        folder_name = folder_name[:100]
    
    return folder_name if folder_name else "default"


def list_algorithms():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ç®—æ³•"""
    auto_register_algorithms()
    print("\nå¯ç”¨ç®—æ³•ï¼š")
    print("=" * 60)
    for name in sorted(ALGORITHMS.keys()):
        print(f"  - {name}")
    print()


def list_datasets():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ•°æ®é›†"""
    print("\nå¯ç”¨æ•°æ®é›†ï¼š")
    print("=" * 60)
    for name in sorted(DATASETS.keys()):
        print(f"  - {name}")
    print()


def list_runbooks():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ runbooks"""
    runbook_dir = Path(__file__).parent / "runbooks"
    print("\nå¯ç”¨ Runbooksï¼ˆæŒ‰ç±»åˆ«åˆ†ç±»ï¼‰ï¼š")
    print("=" * 60)
    
    categories = {}
    for category_dir in sorted(runbook_dir.iterdir()):
        if category_dir.is_dir() and not category_dir.name.startswith('_'):
            yaml_files = list(category_dir.glob("*.yaml"))
            if yaml_files:
                categories[category_dir.name] = [f.stem for f in sorted(yaml_files)]
    
    for category, files in sorted(categories.items()):
        print(f"\n[{category}] ({len(files)} ä¸ª)")
        for file in files:
            print(f"  - {file}")
    print()


def find_runbook_path(runbook_name: str) -> Optional[Path]:
    """
    æŸ¥æ‰¾ runbook æ–‡ä»¶è·¯å¾„
    
    Args:
        runbook_name: runbook åç§°ï¼ˆä¸å¸¦ .yaml åç¼€ï¼‰
        
    Returns:
        å®Œæ•´è·¯å¾„ï¼Œå¦‚æœæ‰¾ä¸åˆ°è¿”å› None
    """
    runbook_dir = Path(__file__).parent / "runbooks"
    
    # å¦‚æœå·²ç»æ˜¯å®Œæ•´è·¯å¾„
    if Path(runbook_name).exists():
        return Path(runbook_name)
    
    # å…ˆåœ¨ runbooks æ ¹ç›®å½•æŸ¥æ‰¾
    yaml_path = runbook_dir / f"{runbook_name}.yaml"
    if yaml_path.exists():
        return yaml_path
    
    # åœ¨æ‰€æœ‰ç±»åˆ«ç›®å½•ä¸­æœç´¢
    for category_dir in runbook_dir.iterdir():
        if category_dir.is_dir():
            yaml_path = category_dir / f"{runbook_name}.yaml"
            if yaml_path.exists():
                return yaml_path
    
    return None




def load_runbook(runbook_path: Path, dataset_name: str = None) -> Tuple[Dict, str]:
    """
    åŠ è½½ runbook æ–‡ä»¶
    
    Args:
        runbook_path: runbook æ–‡ä»¶è·¯å¾„
        dataset_name: æ•°æ®é›†åç§°ï¼ˆç”¨äºé€‰æ‹©é…ç½®ï¼‰
        
    Returns:
        (runbookå­—å…¸, æ•°æ®é›†åç§°) å…ƒç»„
        runbook æ ¼å¼:
        {
            'dataset_name': {
                'max_pts': 1000000,
                1: {'operation': 'startHPC'},
                2: {'operation': 'initial', 'start': 0, 'end': 50000},
                ...
            }
        }
    """
    with open(runbook_path, 'r') as f:
        content = yaml.safe_load(f)
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šæ•°æ®é›†ï¼Œä½¿ç”¨ runbook ä¸­çš„ç¬¬ä¸€ä¸ªæ•°æ®é›†
    if dataset_name is None:
        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªåŒ…å«æ“ä½œçš„æ•°æ®é›†é…ç½®
        for key, value in content.items():
            if isinstance(value, dict) and any(isinstance(k, int) for k in value.keys()):
                dataset_name = key
                break
        
        if dataset_name is None:
            raise ValueError(f"æ— æ³•ä» runbook ä¸­æ‰¾åˆ°æ•°æ®é›†é…ç½®: {runbook_path}")
    
    return content, dataset_name


def run_benchmark(
    algorithm,
    dataset,
    runbook: Dict,
    dataset_name: str,
    k: int = 10,
    run_count: int = 1,
    output_dir: str = "results",
    enable_cache_profiling: bool = False
) -> Tuple[BenchmarkMetrics, List]:
    """
    æ‰§è¡ŒåŸºå‡†æµ‹è¯•ä¸»é€»è¾‘
    é€‚é…æ–°çš„ BenchmarkRunner API
    
    Args:
        algorithm: ç®—æ³•å®ä¾‹
        dataset: æ•°æ®é›†å®ä¾‹
        runbook: runbook æ“ä½œå­—å…¸
        k: kNN çš„ k å€¼
        run_count: è¿è¡Œæ¬¡æ•°ï¼ˆå–æœ€ä½³ç»“æœï¼‰
        output_dir: è¾“å‡ºç›®å½•
        
    Returns:
        (metrics, results): æ€§èƒ½æŒ‡æ ‡å’Œæœç´¢ç»“æœ
    """
    print(f"\n{'='*80}")
    print(f"è¿è¡Œç®—æ³•: {getattr(algorithm, 'name', algorithm.__class__.__name__)}")
    print(f"æ•°æ®é›†: {dataset.short_name()}")
    print(f"è·ç¦»åº¦é‡: {dataset.distance()}")
    print(f"æœç´¢ç±»å‹: {dataset.search_type()}")
    print(f"k å€¼: {k}")
    print(f"è¿è¡Œæ¬¡æ•°: {run_count}")
    print(f"{'='*80}\n")
    
    best_metrics = None
    best_results = None
    best_results_continuous = []
    best_attrs = {}
    best_time = float('inf')
    
    # å¤šæ¬¡è¿è¡Œï¼Œå–æœ€ä½³ç»“æœ
    for run_idx in range(run_count):
        if run_count > 1:
            print(f"\n--- ç¬¬ {run_idx + 1}/{run_count} æ¬¡è¿è¡Œ ---\n")
        
        try:
            # åˆ›å»º runner å®ä¾‹
            runner = BenchmarkRunner(
                algorithm=algorithm,
                dataset=dataset,
                k=k,
                save_timestamps=True,
                output_dir=output_dir,
                enable_cache_profiling=enable_cache_profiling
            )
            
            # æ‰§è¡Œ runbook
            metrics = runner.run_runbook(runbook, dataset_name=dataset_name)
            
            # è®°å½•æœ€ä½³ç»“æœ
            total_time = metrics.total_time if hasattr(metrics, 'total_time') else runner.attrs['totalTime']
            if total_time < best_time:
                best_time = total_time
                best_metrics = metrics
                best_results = runner.all_results
                best_results_continuous = runner.all_results_continuous
                best_attrs = runner.attrs
            
            if run_count > 1:
                print(f"\nç¬¬ {run_idx + 1} æ¬¡è¿è¡Œå®Œæˆï¼Œæ€»æ—¶é—´: {total_time:.2f} ç§’")
        
        except Exception as e:
            print(f"\nâœ— ç¬¬ {run_idx + 1} æ¬¡è¿è¡Œå¤±è´¥: {e}")
            traceback.print_exc()
            if run_idx == run_count - 1:  # æœ€åä¸€æ¬¡å°è¯•
                raise
            continue
    
    if run_count > 1:
        print(f"\næœ€ä½³è¿è¡Œæ—¶é—´: {best_time:.2f} ç§’")
    
    return best_metrics, best_results, best_results_continuous, best_attrs


def get_result_filename(
    dataset: str,
    algorithm: str,
    algorithm_params: Dict[str, Any],
    runbook_name: str,
    output_dir: Optional[Path] = None
) -> str:
    """
    ç”Ÿæˆç»“æœæ–‡ä»¶è·¯å¾„ï¼ŒæŒ‰æ•°æ®é›†/ç®—æ³•/å‚æ•°ç»„ç»‡
    
    æ ¼å¼: results/[dataset]/[algorithm]/[params_folder]
    
    Args:
        dataset: æ•°æ®é›†åç§°
        algorithm: ç®—æ³•åç§°
        algorithm_params: ç®—æ³•å‚æ•°å­—å…¸ï¼ˆåŒ…å« build_params å’Œ query_paramsï¼‰
        runbook_name: runbook åç§°
        output_dir: è¾“å‡ºæ ¹ç›®å½•
    
    Returns:
        ç»“æœæ–‡ä»¶è·¯å¾„ï¼ˆä¸å«æ‰©å±•åï¼‰
    """
    if output_dir is None:
        output_dir = Path('results')
    
    # æ„å»ºç›®å½•ç»“æ„: results/dataset/algorithm/
    parts = [str(output_dir), dataset, algorithm]
    
    # ç”Ÿæˆå‚æ•°æ–‡ä»¶å¤¹å
    params_folder = _generate_params_folder_name(algorithm_params)
    parts.append(params_folder)
    
    return os.path.join(*parts)


def store_results(
    metrics: BenchmarkMetrics,
    results: List,
    results_continuous: List,
    attrs: Dict[str, Any],
    output_dir: Path,
    metadata: Dict[str, Any]
):
    """
    ä¿å­˜æµ‹è¯•ç»“æœåˆ°æ–‡ä»¶ï¼Œå…¼å®¹ big-ann-benchmarks æ ¼å¼
    
    å­˜å‚¨æ ¼å¼:
    - .hdf5: å­˜å‚¨ neighbor ç»“æœï¼ˆæ­£å¼æŸ¥è¯¢ + å‘¨æœŸæ€§æŸ¥è¯¢ï¼‰
    - .csv: å­˜å‚¨å½’ä¸€åŒ–çš„æ€§èƒ½æŒ‡æ ‡ï¼ˆå•è¡Œï¼‰
    - _batchLatency.csv: å­˜å‚¨æ‰¹æ¬¡çº§å»¶è¿Ÿæ•°æ®
    - _batchThroughput.csv: å­˜å‚¨æ‰¹æ¬¡çº§ååé‡æ•°æ®
    - _continuousQueryLatency.csv: å‘¨æœŸæ€§æŸ¥è¯¢å»¶è¿Ÿ
    - _batch_insert_qps.csv: æ‰¹æ¬¡æ’å…¥QPSï¼ˆæ¯æ‰¹æ¬¡çš„æ’å…¥ååé‡ï¼‰
    - _batch_query_qps.csv: æ‰¹æ¬¡æŸ¥è¯¢QPSï¼ˆæ¯æ‰¹æ¬¡çš„æŸ¥è¯¢ååé‡ï¼‰
    - _batch_query_latency.csv: æ‰¹æ¬¡æŸ¥è¯¢å»¶è¿Ÿï¼ˆæ¯æ‰¹æ¬¡çš„æŸ¥è¯¢å»¶è¿Ÿï¼‰
    - _summary.txt: äººç±»å¯è¯»çš„æµ‹è¯•æ‘˜è¦
    
    Args:
        metrics: æ€§èƒ½æŒ‡æ ‡å¯¹è±¡
        results: æ­£å¼æŸ¥è¯¢ç»“æœåˆ—è¡¨ (search operation)
        results_continuous: å‘¨æœŸæ€§æŸ¥è¯¢ç»“æœåˆ—è¡¨ (batch_insert)
        attrs: è¿è¡Œæ—¶å±æ€§å­—å…¸ï¼ˆåŒ…å«å‘¨æœŸæ€§æŸ¥è¯¢æ•°æ®ï¼‰
        output_dir: è¾“å‡ºæ ¹ç›®å½•
        metadata: å…ƒæ•°æ®ä¿¡æ¯
    """
    # ç”Ÿæˆç»“æœè·¯å¾„
    result_path = get_result_filename(
        dataset=metadata['dataset'],
        algorithm=metadata['algorithm'],
        algorithm_params=metadata.get('algorithm_params', {}),
        runbook_name=metadata['runbook'],
        output_dir=output_dir
    )
    
    result_dir = Path(result_path)
    result_dir.mkdir(parents=True, exist_ok=True)
    
    # åŸºç¡€æ–‡ä»¶å
    base_name = Path(result_path).name
    
    # ========== 1. HDF5 æ–‡ä»¶ï¼šå­˜å‚¨ neighbors ç»“æœ ==========
    hdf5_file = result_dir / f"{base_name}.hdf5"
    
    try:
        with h5py.File(hdf5_file, 'w') as f:
            # 1.1 å­˜å‚¨æ­£å¼æŸ¥è¯¢ç»“æœ (search operation)
            if results and len(results) > 0:
                all_neighbors = np.vstack(results) if isinstance(results[0], np.ndarray) else np.array(results)
                f.create_dataset('neighbors', data=all_neighbors, compression='gzip')
                print(f"âœ“ HDF5 æ­£å¼æŸ¥è¯¢ç»“æœ: {hdf5_file} (shape: {all_neighbors.shape})")
            
            # 1.2 å­˜å‚¨å‘¨æœŸæ€§æŸ¥è¯¢ç»“æœ (batch_insert continuous queries)
            if results_continuous and len(results_continuous) > 0:
                all_continuous = np.vstack(results_continuous) if isinstance(results_continuous[0], np.ndarray) else np.array(results_continuous)
                f.create_dataset('neighbors_continuous', data=all_continuous, compression='gzip')
                print(f"âœ“ HDF5 å‘¨æœŸæ€§æŸ¥è¯¢ç»“æœ: (shape: {all_continuous.shape})")
            
            if not results and not results_continuous:
                print(f"âš  æ— æŸ¥è¯¢ç»“æœï¼Œåˆ›å»ºç©º HDF5 æ–‡ä»¶: {hdf5_file}")
    except Exception as e:
        print(f"âš  HDF5 ä¿å­˜å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    # ========== 2. CSV æ–‡ä»¶ï¼šå­˜å‚¨å½’ä¸€åŒ–çš„æ€§èƒ½æŒ‡æ ‡ ==========
    csv_file = result_dir / f"{base_name}.csv"
    
    # æ„å»ºæŒ‡æ ‡å­—å…¸ï¼ˆå½’ä¸€åŒ–ä¸ºå•è¡Œï¼‰
    summary_attrs = {
        'algorithm': metadata['algorithm'],
        'dataset': metadata['dataset'],
        'runbook': metadata['runbook'],
        'k': metadata.get('k', 10),
        'distance': metrics.distance,
        'count': metrics.count,
        'run_count': metadata.get('run_count', 1),
        'mean_query_throughput': float(metrics.mean_query_throughput()) if hasattr(metrics, 'mean_query_throughput') else 0.0,
        'mean_recall': float(metrics.mean_recall()) if hasattr(metrics, 'mean_recall') else 0.0,
        'mean_latency_ms': float(metrics.mean_latency()) if hasattr(metrics, 'mean_latency') else 0.0,
        'p50_latency_ms': float(metrics.p50_latency()) if hasattr(metrics, 'p50_latency') else 0.0,
        'p95_latency_ms': float(metrics.p95_latency()) if hasattr(metrics, 'p95_latency') else 0.0,
        'p99_latency_ms': float(metrics.p99_latency()) if hasattr(metrics, 'p99_latency') else 0.0,
        'mean_insert_throughput': float(metrics.mean_insert_throughput()) if hasattr(metrics, 'mean_insert_throughput') else 0.0,
        'total_time_seconds': float(getattr(metrics, 'total_time', 0.0)),
        'num_searches': getattr(metrics, 'num_searches', 0),
    }
    
    # æ·»åŠ ç®—æ³•å‚æ•°
    if metadata.get('algorithm_params'):
        for key, value in metadata['algorithm_params'].items():
            summary_attrs[f'param_{key}'] = value
    
    # ä¿å­˜ä¸ºå•è¡Œ CSV
    df = pd.DataFrame([summary_attrs])
    df.to_csv(csv_file, index=False)
    print(f"âœ“ CSV æŒ‡æ ‡å·²ä¿å­˜: {csv_file}")
    
    # ========== 3. æ‰¹æ¬¡çº§å»¶è¿Ÿ CSV ==========
    if hasattr(metrics, 'latencies') and len(metrics.latencies) > 0:
        latency_file = result_dir / f"{base_name}_batchLatency.csv"
        latency_df = pd.DataFrame({
            'batch_idx': range(len(metrics.latencies)),
            'latency_us': metrics.latencies
        })
        latency_df.to_csv(latency_file, index=False)
        print(f"âœ“ æ‰¹æ¬¡å»¶è¿Ÿå·²ä¿å­˜: {latency_file}")
    
    # ========== 4. æ‰¹æ¬¡çº§ååé‡ CSV ==========
    if hasattr(metrics, 'throughputs') and len(metrics.throughputs) > 0:
        throughput_file = result_dir / f"{base_name}_batchThroughput.csv"
        throughput_df = pd.DataFrame({
            'batch_idx': range(len(metrics.throughputs)),
            'throughput': metrics.throughputs
        })
        throughput_df.to_csv(throughput_file, index=False)
        print(f"âœ“ æ‰¹æ¬¡ååé‡å·²ä¿å­˜: {throughput_file}")
    
    # ========== 5. å‘¨æœŸæ€§æŸ¥è¯¢å»¶è¿Ÿ CSV ==========
    # æ³¨æ„ï¼šå·²åˆå¹¶åˆ° batch_query_latency.csv ä¸­ï¼Œæ­¤å¤„ä¸å†å•ç‹¬ä¿å­˜
    
    # ========== 6. å‘¨æœŸæ€§æŸ¥è¯¢ç»“æœï¼ˆé‚»å±…IDæ•°ç»„ï¼‰- å·²åœ¨HDF5ä¸­ä¿å­˜ ==========
    # æ³¨æ„ï¼šcontinuousQueryResults å­˜å‚¨çš„æ˜¯é‚»å±… ID æ•°ç»„ï¼Œå·²ä¿å­˜åˆ° HDF5 çš„ neighbors_continuous dataset
    # ä¸å†å•ç‹¬ä¿å­˜ CSVï¼ˆä¸ big-ann-benchmarks ä¸€è‡´ï¼‰
    
    # ========== 7. æ‰¹æ¬¡çº§æ’å…¥QPS (Throughput) CSV ==========
    if 'batchinsertThroughtput' in attrs and len(attrs['batchinsertThroughtput']) > 0:
        insert_qps_file = result_dir / f"{base_name}_batch_insert_qps.csv"
        insert_qps_df = pd.DataFrame({
            'batch_idx': range(len(attrs['batchinsertThroughtput'])),
            'insert_qps': attrs['batchinsertThroughtput']
        })
        insert_qps_df.to_csv(insert_qps_file, index=False)
        print(f"âœ“ æ‰¹æ¬¡æ’å…¥QPSå·²ä¿å­˜: {insert_qps_file}")
    
    # ========== 8. æ‰¹æ¬¡çº§æŸ¥è¯¢QPS CSV ==========
    # æŸ¥è¯¢QPS = æŸ¥è¯¢æ•°é‡ / æŸ¥è¯¢å»¶è¿Ÿï¼ˆç§’ï¼‰
    if 'continuousQueryLatencies' in attrs and len(attrs['continuousQueryLatencies']) > 0:
        query_qps_file = result_dir / f"{base_name}_batch_query_qps.csv"
        # è·å–æ¯æ‰¹æ¬¡çš„æŸ¥è¯¢æ•°é‡ï¼ˆé»˜è®¤ä½¿ç”¨ dataset çš„æŸ¥è¯¢é›†å¤§å°ï¼‰
        queries_per_batch = attrs.get('querySize', 100)
        query_qps_list = []
        batch_indices = []
        
        for idx, latency_seconds in enumerate(attrs['continuousQueryLatencies']):
            # è¿‡æ»¤å¼‚å¸¸å€¼ï¼šåªä¿ç•™æ­£å¸¸çš„å»¶è¿Ÿï¼ˆ>0ä¸”åˆç†èŒƒå›´ï¼‰
            if latency_seconds > 0 and latency_seconds < 3600:  # æœ€å¤š1å°æ—¶
                qps = queries_per_batch / latency_seconds
                query_qps_list.append(qps)
                batch_indices.append(idx)
            else:
                # å¼‚å¸¸å»¶è¿Ÿï¼Œè·³è¿‡è¯¥æ‰¹æ¬¡
                print(f"  âš ï¸  è·³è¿‡å¼‚å¸¸æŸ¥è¯¢å»¶è¿Ÿ: batch_idx={idx}, latency={latency_seconds:.2f}s")
        
        if query_qps_list:
            query_qps_df = pd.DataFrame({
                'batch_idx': batch_indices,
                'query_qps': query_qps_list
            })
            query_qps_df.to_csv(query_qps_file, index=False)
            print(f"âœ“ æ‰¹æ¬¡æŸ¥è¯¢QPSå·²ä¿å­˜: {query_qps_file} ({len(query_qps_list)} ä¸ªæœ‰æ•ˆæ‰¹æ¬¡)")
    
    # ========== 9. Cache Miss CSV ==========
    # ä¿å­˜æ’å…¥çš„cache missç»Ÿè®¡
    if hasattr(metrics, 'cache_miss_per_batch') and len(metrics.cache_miss_per_batch) > 0:
        cache_miss_file = result_dir / f"{base_name}_insert_cache_miss.csv"
        cache_miss_df = pd.DataFrame({
            'batch_idx': range(len(metrics.cache_miss_per_batch)),
            'cache_misses': metrics.cache_miss_per_batch,
            'cache_references': metrics.cache_references_per_batch if hasattr(metrics, 'cache_references_per_batch') else [0] * len(metrics.cache_miss_per_batch),
            'cache_miss_rate': metrics.cache_miss_rate_per_batch if hasattr(metrics, 'cache_miss_rate_per_batch') else [0.0] * len(metrics.cache_miss_per_batch)
        })
        cache_miss_df.to_csv(cache_miss_file, index=False)
        print(f"âœ“ æ’å…¥ Cache Miss å·²ä¿å­˜: {cache_miss_file}")
    
    # ä¿å­˜æŸ¥è¯¢çš„cache missç»Ÿè®¡
    if hasattr(metrics, 'query_cache_miss_per_batch') and len(metrics.query_cache_miss_per_batch) > 0:
        query_cache_miss_file = result_dir / f"{base_name}_query_cache_miss.csv"
        query_cache_miss_df = pd.DataFrame({
            'query_idx': range(len(metrics.query_cache_miss_per_batch)),
            'cache_misses': metrics.query_cache_miss_per_batch,
            'cache_references': metrics.query_cache_references_per_batch if hasattr(metrics, 'query_cache_references_per_batch') else [0] * len(metrics.query_cache_miss_per_batch),
            'cache_miss_rate': metrics.query_cache_miss_rate_per_batch if hasattr(metrics, 'query_cache_miss_rate_per_batch') else [0.0] * len(metrics.query_cache_miss_per_batch)
        })
        query_cache_miss_df.to_csv(query_cache_miss_file, index=False)
        print(f"âœ“ æŸ¥è¯¢ Cache Miss å·²ä¿å­˜: {query_cache_miss_file}")
    
    # ========== 10. æ‰¹æ¬¡çº§æŸ¥è¯¢å»¶è¿Ÿ CSV (æ¯«ç§’) ==========
    if 'continuousQueryLatencies' in attrs and len(attrs['continuousQueryLatencies']) > 0:
        query_latency_file = result_dir / f"{base_name}_batch_query_latency.csv"
        # è½¬æ¢ä¸ºæ¯«ç§’ï¼Œå¹¶è¿‡æ»¤å¼‚å¸¸å€¼
        query_latency_ms = []
        batch_indices = []
        
        for idx, lat_seconds in enumerate(attrs['continuousQueryLatencies']):
            # è¿‡æ»¤å¼‚å¸¸å€¼ï¼šåªä¿ç•™æ­£å¸¸çš„å»¶è¿Ÿï¼ˆ>0ä¸”åˆç†èŒƒå›´ï¼‰
            if lat_seconds > 0 and lat_seconds < 3600:  # æœ€å¤š1å°æ—¶
                query_latency_ms.append(lat_seconds * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
                batch_indices.append(idx)
            else:
                # å¼‚å¸¸å»¶è¿Ÿï¼Œè·³è¿‡
                print(f"  âš ï¸  è·³è¿‡å¼‚å¸¸æŸ¥è¯¢å»¶è¿Ÿ: batch_idx={idx}, latency={lat_seconds:.2f}s")
        
        if query_latency_ms:
            query_latency_df = pd.DataFrame({
                'batch_idx': batch_indices,
                'query_latency_ms': query_latency_ms
            })
            query_latency_df.to_csv(query_latency_file, index=False)
            print(f"âœ“ æ‰¹æ¬¡æŸ¥è¯¢å»¶è¿Ÿå·²ä¿å­˜: {query_latency_file} ({len(query_latency_ms)} ä¸ªæœ‰æ•ˆæ‰¹æ¬¡)")
    
    # ========== 11. ç”Ÿæˆäººç±»å¯è¯»çš„æ‘˜è¦ ==========
    summary_file = result_dir / f"{base_name}_summary.txt"
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("benchmark_anns æµ‹è¯•ç»“æœæ‘˜è¦\n")
        f.write("=" * 80 + "\n\n")
        
        f.write("[æµ‹è¯•é…ç½®]\n")
        f.write(f"ç®—æ³•: {metadata['algorithm']}\n")
        f.write(f"æ•°æ®é›†: {metadata['dataset']}\n")
        f.write(f"Runbook: {metadata['runbook']}\n")
        f.write(f"k å€¼: {metadata.get('k', 10)}\n")
        f.write(f"è¿è¡Œæ¬¡æ•°: {metadata.get('run_count', 1)}\n")
        f.write(f"æµ‹è¯•æ—¶é—´: {metadata.get('timestamp', 'N/A')}\n")
        
        if metadata.get('algorithm_params'):
            f.write(f"\n[ç®—æ³•å‚æ•°]\n")
            for key, value in metadata['algorithm_params'].items():
                f.write(f"  {key}: {value}\n")
        
        f.write("\n[æ€§èƒ½æŒ‡æ ‡]\n")
        f.write(f"å¹³å‡æŸ¥è¯¢ååé‡: {summary_attrs['mean_query_throughput']:.2f} queries/s\n")
        f.write(f"å¹³å‡å¬å›ç‡: {summary_attrs['mean_recall']:.4f}\n")
        f.write(f"å¹³å‡å»¶è¿Ÿ: {summary_attrs['mean_latency_ms']:.2f} ms\n")
        f.write(f"P50 å»¶è¿Ÿ: {summary_attrs['p50_latency_ms']:.2f} ms\n")
        f.write(f"P95 å»¶è¿Ÿ: {summary_attrs['p95_latency_ms']:.2f} ms\n")
        f.write(f"P99 å»¶è¿Ÿ: {summary_attrs['p99_latency_ms']:.2f} ms\n")
        f.write(f"å¹³å‡æ’å…¥ååé‡: {summary_attrs['mean_insert_throughput']:.2f} ops/s\n")
        f.write(f"æ€»æ—¶é—´: {summary_attrs['total_time_seconds']:.2f} ç§’\n")
        f.write(f"æŸ¥è¯¢æ¬¡æ•°: {summary_attrs['num_searches']}\n")
        
        # å‘¨æœŸæ€§æŸ¥è¯¢ç»Ÿè®¡
        if 'continuousQueryLatencies' in attrs and len(attrs['continuousQueryLatencies']) > 0:
            f.write(f"\n[å‘¨æœŸæ€§æŸ¥è¯¢ç»Ÿè®¡]\n")
            f.write(f"å‘¨æœŸæ€§æŸ¥è¯¢æ¬¡æ•°: {len(attrs['continuousQueryLatencies'])}\n")
            cq_latencies_ms = [lat * 1000 for lat in attrs['continuousQueryLatencies']]
            f.write(f"å¹³å‡å»¶è¿Ÿ: {np.mean(cq_latencies_ms):.2f} ms\n")
            f.write(f"æŸ¥è¯¢ç»“æœå·²ä¿å­˜åˆ° HDF5: neighbors_continuous dataset\n")
        
        # æ‰¹æ¬¡çº§æ•°æ®ç»Ÿè®¡
        if 'batchinsertThroughtput' in attrs and len(attrs['batchinsertThroughtput']) > 0:
            f.write(f"\n[æ‰¹æ¬¡çº§æ•°æ®ç»Ÿè®¡]\n")
            f.write(f"æ‰¹æ¬¡æ•°é‡: {len(attrs['batchinsertThroughtput'])}\n")
            f.write(f"å¹³å‡æ’å…¥QPS: {np.mean(attrs['batchinsertThroughtput']):.2f} ops/s\n")
            f.write(f"æœ€å¤§æ’å…¥QPS: {np.max(attrs['batchinsertThroughtput']):.2f} ops/s\n")
            f.write(f"æœ€å°æ’å…¥QPS: {np.min(attrs['batchinsertThroughtput']):.2f} ops/s\n")
            f.write(f"æ‰¹æ¬¡æ’å…¥QPSå·²ä¿å­˜åˆ°: {base_name}_batch_insert_qps.csv\n")
            
            if 'continuousQueryLatencies' in attrs and len(attrs['continuousQueryLatencies']) > 0:
                f.write(f"æ‰¹æ¬¡æŸ¥è¯¢QPSå·²ä¿å­˜åˆ°: {base_name}_batch_query_qps.csv\n")
                f.write(f"æ‰¹æ¬¡æŸ¥è¯¢å»¶è¿Ÿå·²ä¿å­˜åˆ°: {base_name}_batch_query_latency.csv\n")
        
        # Cache Miss ç»Ÿè®¡
        if hasattr(metrics, 'cache_miss_per_batch') and len(metrics.cache_miss_per_batch) > 0:
            f.write(f"\n[Cache Miss ç»Ÿè®¡]\n")
            f.write(f"æ’å…¥æ‰¹æ¬¡æ•°é‡: {len(metrics.cache_miss_per_batch)}\n")
            f.write(f"æ’å…¥å¹³å‡ Cache Misses: {np.mean(metrics.cache_miss_per_batch):,.0f}\n")
            if hasattr(metrics, 'cache_miss_rate_per_batch') and len(metrics.cache_miss_rate_per_batch) > 0:
                f.write(f"æ’å…¥å¹³å‡ Cache Miss Rate: {np.mean(metrics.cache_miss_rate_per_batch):.2%}\n")
            f.write(f"æ’å…¥ Cache Miss å·²ä¿å­˜åˆ°: {base_name}_insert_cache_miss.csv\n")
        
        if hasattr(metrics, 'query_cache_miss_per_batch') and len(metrics.query_cache_miss_per_batch) > 0:
            f.write(f"\næŸ¥è¯¢æ¬¡æ•°: {len(metrics.query_cache_miss_per_batch)}\n")
            f.write(f"æŸ¥è¯¢å¹³å‡ Cache Misses: {np.mean(metrics.query_cache_miss_per_batch):,.0f}\n")
            if hasattr(metrics, 'query_cache_miss_rate_per_batch') and len(metrics.query_cache_miss_rate_per_batch) > 0:
                f.write(f"æŸ¥è¯¢å¹³å‡ Cache Miss Rate: {np.mean(metrics.query_cache_miss_rate_per_batch):.2%}\n")
            f.write(f"æŸ¥è¯¢ Cache Miss å·²ä¿å­˜åˆ°: {base_name}_query_cache_miss.csv\n")
        
        f.write("\n" + "=" * 80 + "\n")
    
    print(f"âœ“ æµ‹è¯•æ‘˜è¦å·²ä¿å­˜: {summary_file}")
    print(f"\nç»“æœç›®å½•: {result_dir}")



def print_results_summary(metrics):
    """æ‰“å°ç»“æœæ‘˜è¦åˆ°æ§åˆ¶å°"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•ç»“æœæ‘˜è¦")
    print("=" * 80)
    print(f"ç®—æ³•: {metrics.algorithm_name}")
    print(f"æ•°æ®é›†: {metrics.dataset_name}")
    
    # è®¡ç®—æ€»æ—¶é—´ï¼ˆæ”¯æŒä¸åŒçš„æ ¼å¼ï¼‰
    total_time = float(metrics.total_time) if hasattr(metrics, 'total_time') else 0
    if total_time < 1000:  # å¦‚æœå°äº 1000ï¼Œå¯èƒ½æ˜¯ç§’
        print(f"æ€»æ—¶é—´: {total_time:.2f} ç§’")
    else:
        print(f"æ€»æ—¶é—´: {total_time/1e6:.2f} ç§’")
    
    print(f"æŸ¥è¯¢æ¬¡æ•°: {metrics.num_searches}")
    print(f"\næ€§èƒ½æŒ‡æ ‡:")
    
    # if hasattr(metrics, 'mean_recall'):
    #     print(f"  å¹³å‡å¬å›ç‡: {metrics.mean_recall():.4f}")
    if hasattr(metrics, 'mean_query_throughput'):
        print(f"  å¹³å‡æŸ¥è¯¢ååé‡: {metrics.mean_query_throughput():.2f} queries/s")
    if hasattr(metrics, 'mean_insert_throughput'):
        print(f"  å¹³å‡æ’å…¥ååé‡: {metrics.mean_insert_throughput():.2f} ops/s")
    if hasattr(metrics, 'mean_latency'):
        print(f"  å¹³å‡å»¶è¿Ÿ: {metrics.mean_latency():.2f} ms")
    if hasattr(metrics, 'p50_latency'):
        print(f"  P50 å»¶è¿Ÿ: {metrics.p50_latency():.2f} ms")
    if hasattr(metrics, 'p95_latency'):
        print(f"  P95 å»¶è¿Ÿ: {metrics.p95_latency():.2f} ms")
    if hasattr(metrics, 'p99_latency'):
        print(f"  P99 å»¶è¿Ÿ: {metrics.p99_latency():.2f} ms")
    
    if hasattr(metrics, 'maintenance_budget_used'):
        print(f"  ç»´æŠ¤é¢„ç®—ä½¿ç”¨: {metrics.maintenance_budget_used/1e6:.2f} ç§’")
    
    print("=" * 80 + "\n")


def main():
    parser = argparse.ArgumentParser(
        description='benchmark_anns - æµå¼ç´¢å¼•åŸºå‡†æµ‹è¯•ä¸»ç¨‹åº',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•ï¼š
  # è¿è¡ŒåŸºå‡†æµ‹è¯•
  python run_benchmark.py --algorithm faiss_hnsw --dataset sift --runbook batch_2500
  
  # æŒ‡å®šç®—æ³•å‚æ•°
  python run_benchmark.py --algorithm faiss_hnsw --dataset sift --runbook rate_10000 \\
      --algo-params '{"M": 32, "efConstruction": 200, "efSearch": 100}'
  
  # æŒ‡å®šè¾“å‡ºç›®å½•
  python run_benchmark.py --algorithm faiss_hnsw --dataset sift --runbook batch_2500 \\
      --output results/exp1
  
  # å¯ç”¨ cache miss æµ‹é‡
  python run_benchmark.py --algorithm faiss_hnsw --dataset sift --runbook general_experiment \\
      --enable-cache-miss
  
  # åˆ—å‡ºå¯ç”¨é€‰é¡¹
  python run_benchmark.py --list-algorithms
  python run_benchmark.py --list-datasets
  python run_benchmark.py --list-runbooks
        """
    )
    
    # åˆ—è¡¨é€‰é¡¹
    parser.add_argument('--list-algorithms', action='store_true', 
                       help='åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ç®—æ³•')
    parser.add_argument('--list-datasets', action='store_true',
                       help='åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ•°æ®é›†')
    parser.add_argument('--list-runbooks', action='store_true',
                       help='åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ runbooks')
    
    # ä¸»è¦å‚æ•°
    parser.add_argument('--algorithm', type=str,
                       help='ç®—æ³•åç§°ï¼ˆå¦‚: faiss_hnsw, candy_lshapgï¼‰')
    parser.add_argument('--dataset', type=str,
                       help='æ•°æ®é›†åç§°ï¼ˆå¦‚: sift, random-xsï¼‰')
    parser.add_argument('--runbook', type=str,
                       help='Runbook åç§°ï¼ˆå¦‚: batch_2500, rate_10000ï¼‰')
    
    # å¯é€‰å‚æ•°
    parser.add_argument('--algo-params', type=str, default='{}',
                       help='ç®—æ³•å‚æ•°ï¼ˆJSON æ ¼å¼å­—ç¬¦ä¸²ï¼‰')
    parser.add_argument('--k', type=int, default=10,
                       help='kNN æŸ¥è¯¢çš„ k å€¼ï¼ˆé»˜è®¤: 10ï¼‰')
    parser.add_argument('--runs', type=int, default=1,
                       help='è¿è¡Œæ¬¡æ•°ï¼Œå–æœ€ä½³ç»“æœï¼ˆé»˜è®¤: 1ï¼‰')
    parser.add_argument('--output', type=str, default='results',
                       help='è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: resultsï¼‰')
    parser.add_argument('--rebuild', action='store_true',
                       help='å¼ºåˆ¶é‡å»ºç´¢å¼•ï¼ˆå³ä½¿ç´¢å¼•æ–‡ä»¶å­˜åœ¨ï¼‰')
    parser.add_argument('--enable-cache-profiling', action='store_true',
                       help='å¯ç”¨ cache miss æ€§èƒ½ç›‘æµ‹ï¼ˆéœ€è¦ perf å·¥å…·æ”¯æŒï¼‰')
    parser.add_argument('--no-save', action='store_true',
                       help='ä¸ä¿å­˜ç»“æœæ–‡ä»¶')
        
    args = parser.parse_args()
    
    # å¤„ç†åˆ—è¡¨è¯·æ±‚
    if args.list_algorithms:
        list_algorithms()
        return
    
    if args.list_datasets:
        list_datasets()
        return
    
    if args.list_runbooks:
        list_runbooks()
        return
    
    # éªŒè¯å¿…éœ€å‚æ•°
    if not all([args.algorithm, args.dataset, args.runbook]):
        parser.error("å¿…é¡»æŒ‡å®š --algorithm, --dataset å’Œ --runbookï¼ˆæˆ–ä½¿ç”¨ --list-* é€‰é¡¹ï¼‰")
    
    # è§£æå‘½ä»¤è¡Œç®—æ³•å‚æ•°
    try:
        cli_algo_params = json.loads(args.algo_params)
    except json.JSONDecodeError as e:
        print(f"é”™è¯¯: æ— æ³•è§£æç®—æ³•å‚æ•° JSON: {e}")
        sys.exit(1)
    
    # è‡ªåŠ¨æ³¨å†Œç®—æ³•
    auto_register_algorithms()
    
    # è·å–æ‰€æœ‰å‚æ•°ç»„åˆ
    if cli_algo_params:
        # å¦‚æœå‘½ä»¤è¡ŒæŒ‡å®šäº†å‚æ•°ï¼Œåªè¿è¡Œè¿™ä¸€ç»„
        param_combinations = [{'build_params': cli_algo_params, 'query_params': {}}]
    else:
        # ä»é…ç½®æ–‡ä»¶è·å–æ‰€æœ‰å‚æ•°ç»„åˆ
        param_combinations = get_all_algorithm_param_combinations(args.algorithm, args.dataset)
    
    total_combinations = len(param_combinations)
    
    print("\n" + "=" * 80)
    print("benchmark_anns æµå¼ç´¢å¼•åŸºå‡†æµ‹è¯•")
    print("=" * 80)
    print(f"ç®—æ³•: {args.algorithm}")
    print(f"æ•°æ®é›†: {args.dataset}")
    print(f"Runbook: {args.runbook}")
    print(f"k å€¼: {args.k}")
    print(f"å‚æ•°ç»„åˆæ•°: {total_combinations}")
    print("=" * 80 + "\n")
    
    # 1. åŠ è½½æ•°æ®é›†ï¼ˆåªåŠ è½½ä¸€æ¬¡ï¼‰
    print("[1/4] åŠ è½½æ•°æ®é›†...")
    try:
        dataset = get_dataset(args.dataset)
        print(f"âœ“ æ•°æ®é›†åŠ è½½æˆåŠŸ: {dataset.short_name()}")
        print(f"  - è·ç¦»åº¦é‡: {dataset.distance()}")
        print(f"  - å‘é‡ç»´åº¦: {dataset.d}")
    except Exception as e:
        print(f"âœ— æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
        sys.exit(1)
    
    # 2. åŠ è½½ runbookï¼ˆåªåŠ è½½ä¸€æ¬¡ï¼‰
    print("\n[2/4] åŠ è½½ Runbook...")
    try:
        runbook_path = find_runbook_path(args.runbook)
        if not runbook_path:
            print(f"âœ— æ‰¾ä¸åˆ° runbook: {args.runbook}")
            print("ä½¿ç”¨ --list-runbooks æŸ¥çœ‹å¯ç”¨çš„ runbooks")
            sys.exit(1)
        
        dataset_arg = args.dataset if hasattr(args, 'dataset') and args.dataset else None
        runbook, dataset_name = load_runbook(runbook_path, dataset_name=dataset_arg)
        
        if dataset_name in runbook:
            dataset_config = runbook[dataset_name]
            op_count = sum(1 for k in dataset_config.keys() if isinstance(k, int))
            print(f"âœ“ Runbook åŠ è½½æˆåŠŸ: {runbook_path}")
            print(f"  - æ•°æ®é›†: {dataset_name}")
            print(f"  - æ“ä½œæ­¥éª¤: {op_count}")
            if 'max_pts' in dataset_config:
                print(f"  - æœ€å¤§æ•°æ®ç‚¹: {dataset_config['max_pts']}")
        else:
            print(f"âœ“ Runbook åŠ è½½æˆåŠŸ: {runbook_path}")
    except Exception as e:
        print(f"âœ— Runbook åŠ è½½å¤±è´¥: {e}")
        traceback.print_exc()
        sys.exit(1)
    
    # 3. éå†æ‰€æœ‰å‚æ•°ç»„åˆæ‰§è¡Œæµ‹è¯•
    print(f"\n[3/4] æµ‹è¯• ({total_combinations} ç»„å‚æ•°)...")
    
    all_results = []
    for combo_idx, param_combo in enumerate(param_combinations, 1):
        build_params = param_combo.get('build_params', {})
        query_params = param_combo.get('query_params', {})
        
        # ç”Ÿæˆç®€æ´çš„å‚æ•°æè¿°
        param_desc = _generate_params_folder_name({'build_params': build_params, 'query_params': query_params})
        
        print(f"\n{'â•' * 70}")
        print(f"  å‚æ•°ç»„åˆ [{combo_idx}/{total_combinations}]: {param_desc}")
        print(f"{'â•' * 70}")
        
        # æ‰“å°å…³é”®æ„å»ºå‚æ•°
        if build_params:
            print(f"  ğŸ“¦ æ„å»ºå‚æ•°:")
            flat_build = _extract_key_params(build_params)
            for key, value in sorted(flat_build.items()):
                print(f"      â€¢ {key}: {value}")
        
        # æ‰“å°å…³é”®æŸ¥è¯¢å‚æ•°  
        if query_params:
            print(f"  ğŸ” æŸ¥è¯¢å‚æ•°:")
            flat_query = _extract_key_params(query_params)
            for key, value in sorted(flat_query.items()):
                print(f"      â€¢ {key}: {value}")
        
        print(f"{'â”€' * 70}")
        
        try:
            # åˆå§‹åŒ–ç®—æ³•ï¼ˆæ¯æ¬¡ç”¨ä¸åŒå‚æ•°ï¼‰
            # å°† build_params åŒ…è£…æˆ index_params ä¼ ç»™ç®—æ³•æ„é€ å‡½æ•°
            algo_kwargs = {'index_params': build_params} if build_params else {}
            algorithm = get_algorithm(args.algorithm, dataset=args.dataset, **algo_kwargs)
            
            # å¦‚æœç®—æ³•æ”¯æŒè®¾ç½®æŸ¥è¯¢å‚æ•°
            if hasattr(algorithm, 'set_query_arguments') and query_params:
                algorithm.set_query_arguments(query_params)
            
            # æ‰§è¡Œæµ‹è¯•
            metrics, best_results, best_results_continuous, best_attrs = run_benchmark(
                algorithm=algorithm,
                dataset=dataset,
                runbook=runbook,
                dataset_name=dataset_name,
                k=args.k,
                run_count=args.runs,
                output_dir=args.output,
                enable_cache_profiling=args.enable_cache_profiling
            )
            print(f"âœ“ å‚æ•°ç»„åˆ [{combo_idx}] æµ‹è¯•å®Œæˆ")
            
            # ä¿å­˜ç»“æœ
            if not args.no_save:
                actual_algo_params = {
                    'build_params': build_params,
                    'query_params': query_params
                }
                
                metadata = {
                    'algorithm': args.algorithm,
                    'algorithm_params': actual_algo_params,
                    'dataset': args.dataset,
                    'runbook': args.runbook,
                    'k': args.k,
                    'run_count': args.runs,
                    'timestamp': datetime.now().isoformat(),
                    'param_combo_index': combo_idx,
                    'total_param_combos': total_combinations,
                }
                
                output_dir = Path(args.output)
                store_results(metrics, best_results, best_results_continuous, best_attrs, output_dir, metadata)
                print(f"âœ“ å‚æ•°ç»„åˆ [{combo_idx}] ç»“æœå·²ä¿å­˜")
            
            all_results.append({
                'combo_idx': combo_idx,
                'build_params': build_params,
                'query_params': query_params,
                'metrics': metrics,
                'success': True
            })
            
            # æ‰“å°å•æ¬¡ç»“æœæ‘˜è¦
            print_results_summary(metrics)
            
        except Exception as e:
            print(f"âœ— å‚æ•°ç»„åˆ [{combo_idx}] æ‰§è¡Œå¤±è´¥: {e}")
            traceback.print_exc()
            all_results.append({
                'combo_idx': combo_idx,
                'build_params': build_params,
                'query_params': query_params,
                'error': str(e),
                'success': False
            })
    
    # 4. æ‰“å°æ€»ä½“æ‘˜è¦
    print(f"\n[4/4] æµ‹è¯•å®Œæˆæ‘˜è¦")
    print("=" * 80)
    success_count = sum(1 for r in all_results if r['success'])
    fail_count = len(all_results) - success_count
    print(f"æ€»å‚æ•°ç»„åˆæ•°: {total_combinations}")
    print(f"æˆåŠŸ: {success_count}, å¤±è´¥: {fail_count}")
    
    if success_count > 0:
        print("\næˆåŠŸçš„æµ‹è¯•ç»“æœ:")
        for result in all_results:
            if result['success']:
                metrics = result['metrics']
                build_info = _generate_params_folder_name({'build_params': result['build_params'], 'query_params': result['query_params']})
                recall = metrics.mean_recall() if hasattr(metrics, 'mean_recall') else 'N/A'
                qps = metrics.mean_qps() if hasattr(metrics, 'mean_qps') else 'N/A'
                print(f"  [{result['combo_idx']}] {build_info}")
                if recall != 'N/A':
                    print(f"      Recall: {recall:.4f}, QPS: {qps:.2f}")
    
    print("=" * 80)
    print("\næµ‹è¯•å®Œæˆï¼")


if __name__ == '__main__':
    main()
